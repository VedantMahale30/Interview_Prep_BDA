	Hive Preparation : 
-------------------------------------------------------------------------------------------------------------
Hive is a data warehouse system which is used to analyze structured data. It is built on the top of Hadoop. It was developed by Facebook.
Hive provides the functionality of reading, writing, and managing large datasets residing in distributed storage. 
It runs  like queries called HQL (Hive query language) which gets internally converted to MapReduce jobs.
Using Hive, we can skip the requirement of the traditional approach of writing complex MapReduce programs. 
Hive supports Data Definition Language (DDL), Data Manipulation Language (DML), and User Defined Functions (UDF).

--------------------------------------------------------------------------------------------------------------
	Features of Hive
--------------------------------------------------------------------------------------------------------------
Hive is fast and scalable.
It provides -like queries (i.e., HQL) that are implicitly transformed to MapReduce or Spark jobs.
It is capable of analyzing large datasets stored in HDFS.
It allows different storage types such as plain text, RCFile, and HBase.
It uses indexing to accelerate queries.
It can operate on compressed data stored in the Hadoop ecosystem.
It supports user-defined functions (UDFs) where user can provide its functionality.

--------------------------------------------------------------------------------------------------------------
	Limitations of Hive
--------------------------------------------------------------------------------------------------------------
Hive is not capable of handling real-time data.
It is not designed for online transaction processing.
Hive queries contain high latency.

--------------------------------------------------------------------------------------------------------------
	Components of Hive Architecture
--------------------------------------------------------------------------------------------------------------
a. User Interface (UI)
		Provides interfaces for users to interact with Hive.
		Types of User Interfaces:
			Command Line Interface (CLI): The most common method for interacting with Hive.
			Web Interface (Hive Web UI): A web-based graphical user interface for submitting queries.
			ODBC/JDBC Drivers: Allows integration with external applications (e.g., BI tools like Tableau).
			
b. Driver
		Acts as the main controller or coordinator in Hive architecture.
		Manages the lifecycle of a HiveQL query:
			Parses the query.
			Checks query syntax and semantics.
			Optimizes the query.
			Executes the query by interacting with the compiler and execution engine.
c. Compiler
		Converts HiveQL queries into execution plans.
		Key Functions:
			Parse HiveQL queries to generate Abstract Syntax Trees (ASTs).
			Validate the query against the metadata (schema and table details).
			Generate logical execution plans optimized for distributed systems.
			Translate logical plans into physical plans using MapReduce, Tez, or Spark.
			
d. Optimizer
		Optimizes the execution plan to reduce resource usage and improve query performance.
		Techniques include:
			Predicate Pushdown: Filters data early to minimize data scanned.
			Partition Pruning: Scans only the relevant partitions of a table.
			Join Optimization: Reorganizes query execution for faster joins.
			Cost-Based Optimization (CBO): Chooses the best query execution plan based on resource costs.
			
e. Execution Engine
		Responsible for executing the physical plans generated by the compiler.
		Works with Hadoop's execution frameworks:
			MapReduce: Default execution engine for batch processing.
			Apache Tez: A more efficient, DAG-based engine for faster execution.
			Apache Spark: Provides in-memory processing for real-time or interactive queries.
		
		The engine communicates with the Hadoop cluster for distributed data processing.
			
f. Metastore
		Central repository for storing metadata about Hive tables, databases, and partitions.
		Metadata includes:
			Table schemas, column types, and data locations.
			Partition information and table statistics.
			Storage formats (e.g., ORC, Parquet, Text).
		Implementation:
			Typically uses a relational database like My or Postgre.
			Hive queries interact with the metastore to retrieve schema and storage details.
		Ensures data consistency and query efficiency.
			
g. HDFS/Storage
		The actual data queried by Hive resides in Hadoop’s storage layer.
		Supports multiple file formats:
			Text, Sequence, ORC, Parquet, Avro.
		Hive queries leverage Hadoop's distributed storage and processing capabilities.
--------------------------------------------------------------------------------------------------------------
	Workflow of Query Execution in Hive
--------------------------------------------------------------------------------------------------------------
	Step 1: Query Submission
		A user submits a HiveQL query through the CLI, Web UI, or a JDBC/ODBC client.
	Step 2: Parsing
		The query is parsed by the Driver to check for syntax errors.
		An Abstract Syntax Tree (AST) is generated.
	Step 3: Semantic Analysis
		The Driver performs semantic analysis to validate the query against the metadata in the Metastore (e.g., checking if tables exist).
	Step 4: Logical Plan Creation
		The Compiler creates a logical execution plan for the query.
		The Optimizer refines this plan to minimize resource usage.
	Step 5: Physical Plan Creation
		The logical plan is converted into a physical execution plan.
		The execution plan is customized for the chosen execution engine (MapReduce, Tez, or Spark).
	Step 6: Execution
		The Execution Engine translates the physical plan into MapReduce/Tez/Spark jobs.
		The jobs are executed in the Hadoop cluster, interacting with HDFS to read and process data.
	Step 7: Result Retrieval
		The processed data is written back to HDFS or sent to the user as query results.
--------------------------------------------------------------------------------------------------------------
 Hive Architecture Diagram
--------------------------------------------------------------------------------------------------------------
+---------------------+      +-----------------+
|     User Interface  | ---> |     Driver      |
+---------------------+      +-----------------+
                                    |
                                    v
                          +-------------------+
                          |     Compiler      |
                          +-------------------+
                                    |
                                    v
                          +-------------------+
                          |     Optimizer     |
                          +-------------------+
                                    |
                                    v
                          +-------------------+
                          | Execution Engine  |
                          +-------------------+
                                    |
             +---------------------------------------+
             |                                       |
+---------------------------+       +---------------------------+
|          Metastore        |       |          HDFS            |
+---------------------------+       +---------------------------+

--------------------------------------------------------------------------------------------------------------
	HiveQL (Hive Query Language)
--------------------------------------------------------------------------------------------------------------
HiveQL is Hive’s query language, designed to resemble . This makes it easier for those familiar with  to interact with Hadoop data. However, HiveQL is optimized for large-scale data processing and works with Hadoop’s distributed file system (HDFS).

Basic Syntax:
		SELECT: Retrieve data from Hive tables.			
		CREATE: Define a new table or database.
		DROP: Remove tables or databases.
		ALTER: Modify existing tables, such as adding or removing columns.

Example Query: Suppose you want to get the total sales by region from a sales table:

		SELECT region, SUM(sales_amount)		
		FROM sales_data		
		WHERE sale_date BETWEEN '2024-01-01' AND '2024-12-31'		
		GROUP BY region;
		
		This query will retrieve the sum of sales for each region for the year 2024.
--------------------------------------------------------------------------------------------------------------
	Join Operations: 
--------------------------------------------------------------------------------------------------------------
Hive supports INNER JOIN, LEFT JOIN, RIGHT JOIN, and OUTER JOIN operations, similar to . 
However, since Hive queries are translated into MapReduce jobs (or Tez/Spark jobs), joins on very large datasets may be slower 
compared to traditional relational databases.

Example (INNER JOIN):
	SELECT a.order_id, b.product_name
	FROM orders a
	JOIN products b
	ON a.product_id = b.product_id;
	
Limitations:
	Hive is not designed for low-latency queries like a traditional RDBMS (Relational Database Management System). It's more suited for large-scale batch processing.
	Hive queries, especially JOINs and GROUP BY, can take longer to execute compared to regular  databases because of the underlying MapReduce framework (unless optimized).

--------------------------------------------------------------------------------------------------------------	
	Hive Data Types
--------------------------------------------------------------------------------------------------------------
	Hive supports several data types, including both primitive and complex types. 
	Understanding these types is essential for defining schemas and performing queries correctly.

	Primitive Data Types:
			INT: Integer values, e.g., 100.
			BIGINT: Large integer values, e.g., 10000000000.
			STRING: Sequence of characters, e.g., 'Hello World'.
			FLOAT and DOUBLE: Floating point numbers, e.g., 12.34 and 12.34e2.
			BOOLEAN: Represents TRUE or FALSE.
			DATE/TIMESTAMP: Used for date and time values. DATE stores only the date, while TIMESTAMP stores both date and time.
	Complex Data Types:
			A) ARRAY: A collection of elements of the same type. You can define an array for fields that contain lists of values.
					Example: ARRAY<STRING> represents a list of strings.
				Query Example:
					SELECT customer_id, purchased_items
					FROM orders 
					WHERE customer_id = 123;
					Where purchased_items is an array.
			
			B) MAP: A key-value pair structure, useful for storing pairs of data, such as MAP<STRING, INT>, representing a mapping of strings to integers.
			
			C) STRUCT: A collection of fields that may have different data types. This is used to define complex objects like records or tuples.
					Example: STRUCT<name STRING, age INT> could represent a person’s name and age as a single column.
			
			D) UNIONTYPE: Allows for defining a column that can have multiple possible data types.
					Example (using complex data types):
							
							CREATE TABLE orders (
								order_id INT,
								customer_info STRUCT<name: STRING, age: INT>,
								items ARRAY<STRING>
							);
							
							This table will store each order's ID, customer’s information (name and age), and a list of items in an array.
--------------------------------------------------------------------------------------------------------------
	Hive Tables: Managed vs External
--------------------------------------------------------------------------------------------------------------
Hive tables are the primary way to organize and query data in Hive. Understanding the distinction between Managed (Internal) and External tables is crucial for designing efficient data processing systems in Hive. Let's dive deeper into these two types of tables and their implications.

1. Managed Tables (Internal Tables)
Definition
A managed table in Hive is fully controlled by Hive. Hive manages both the metadata (schema) and the actual data. By default, when you create a table in Hive without explicitly specifying it as external, it is treated as a managed table.

		Storage Location
			Data for managed tables is stored in Hive’s default warehouse directory, typically:
				/user/hive/warehouse/<database_name>.db/<table_name>/
			You can also specify a custom location for a managed table using the LOCATION clause.

		Data Management
			Hive takes full ownership of the data.
			If a managed table is dropped, both the metadata and the data are deleted from HDFS.
			
		Use Cases
			When Hive is the primary owner and manager of the data.
			For temporary or intermediate tables where data retention isn't critical.
			When you want Hive to handle all aspects of the table lifecycle, including cleanup.
		
		Advantages
			Simplifies data management as Hive handles everything.
			Ensures that unnecessary data isn’t left behind if the table is dropped.
		Disadvantages
			Deleting a managed table accidentally can result in permanent data loss.
		
		Example
			CREATE TABLE managed_table (
				id INT,
				name STRING,
				age INT
			)
			STORED AS PARQUET;
			
			
Data for this table will be stored in Hive’s warehouse directory.
Dropping this table (DROP TABLE managed_table;) will delete both the table metadata and the data.


2. External Tables
		Definition
				An external table in Hive allows you to manage the data separately from Hive. 
				Hive only manages the schema and does not own the actual data.
		
		Storage Location
				
				Data for external tables resides outside Hive’s warehouse directory.
				The location must be explicitly specified during table creation using the LOCATION clause.
		
		Data Management
				
				Hive does not delete the data when an external table is dropped.
				The responsibility for managing and cleaning up the data lies with the user.
		
		Use Cases
				
				When the data is shared between multiple tools or systems (e.g., Spark, Pig).
				For datasets where Hive is not the sole owner or where the data must persist independently of the table.
				For large, immutable datasets stored in HDFS, AWS S3, or other storage systems.
		
		Advantages
				
				Prevents accidental data deletion when dropping tables.
				Useful for integrating Hive with existing data pipelines or external systems.
		
		Disadvantages
				
				The user is responsible for managing the data, which can lead to complexities if not handled properly.
				Example
				CREATE EXTERNAL TABLE external_table (
					id INT,
					name STRING,
					age INT
				)
				STORED AS PARQUET
				LOCATION '/user/data/external/';
				
		Data for this table resides in the specified location: /user/data/external/.
		
		Dropping this table (DROP TABLE external_table;) will delete only the schema, not the data.
		

3. Key Differences Between Managed and External Tables
|---------------------------|----------------------------------------------------|--------------------------------------------------|
|   Aspect                  | Managed Table                                 	 |	External Table                                  |
|---------------------------|----------------------------------------------------|--------------------------------------------------|
| Ownership             	| Hive owns both schema and data.                    | Hive owns only the schema, not the data.         |
| Storage Location      	| Default warehouse directory (unless specified).    | Custom location must be specified.               |
| Data Management       	| Hive manages and deletes data when dropped.        | User is responsible for managing data.           |
| Deletion Impact       	| Deletes both metadata and data.                    | Deletes only metadata; data remains intact.      |
| Use Cases             	| For temporary or Hive-controlled data.             | For shared or externally managed data sources.   |
|---------------------------|----------------------------------------------------|--------------------------------------------------|


4 . How Hive Manages Data Internally
			Managed Tables
				Stores data in the Hive warehouse directory.
				Data is tightly coupled with Hive, and any operation (like a table drop) affects both metadata and the underlying data.
			External Tables
				Stores data outside the warehouse directory.
				Metadata resides in Hive, but the actual data remains decoupled, allowing for integration with other systems or frameworks.




--------------------------------------------------------------------------------------------------------------
	Hive Partitioning and Bucketing: Detailed Explanation with Directory Structure
--------------------------------------------------------------------------------------------------------------

1. Partitioning in Hive
Partitioning is a technique in Hive to divide a table into smaller parts based on the values of one or more columns, referred to as partition columns. 
Each partition is stored as a separate directory in the file system, making data retrieval faster by scanning only relevant partitions.
	
	Key Features:
			- Data is organized based on partition columns.
			- Queries are optimized by scanning only the necessary partitions.
			- Reduces query latency for large datasets.


Directory Structure:

For a table partitioned by `year` and `month`, the data is stored in a hierarchical directory structure.
				Example:
					/user/hive/warehouse/sales_data/
					year=2024/
						month=01/
						part-00000
						part-00001
						month=02/
						part-00000
					year=2023/
						month=12/
						part-00000

#Example Query:
				CREATE TABLE sales_data (
					id INT,
					amount FLOAT
				)
				PARTITIONED BY (year INT, month STRING)
				STORED AS PARQUET;
				
- To load data into a partition:
LOAD DATA INPATH '/data/2024-01' INTO TABLE sales_data PARTITION (year=2024, month='01');


- Querying specific partitions:
SELECT * FROM sales_data WHERE year = 2024 AND month = '01';


#Advantages of Partitioning:
			- Faster query execution for partition-based filters.
			- Efficient storage and retrieval for time-series or categorized data.

#Disadvantages:
			- Over-partitioning can lead to too many small files, increasing overhead.
			- Manual management of partitions can be cumbersome for dynamic datasets.

2. Bucketing in Hive

Bucketing divides data into fixed-size files (buckets) based on the hash of a column, ensuring even distribution of data. 
Bucketing is complementary to partitioning and is used for efficient sampling and joins.

#Key Features:
			- Data is grouped into buckets using a hash function.
			- Buckets are evenly distributed to optimize processing.
			- Ideal for join operations and sampling large datasets.


Directory Structure:
For a table bucketed by `id` into 4 buckets, the data is stored as files within the table’s directory:
			Example:
				/user/hive/warehouse/customer_data/
				000000_0
				000001_0
				000002_0
				000003_0
				

#Example Query:
				CREATE TABLE customer_data (
					id INT,
					name STRING,
					age INT
				)
				CLUSTERED BY (id) INTO 4 BUCKETS
				STORED AS ORC;

- To load data into a bucketed table:
		SET hive.enforce.bucketing = true;
		INSERT INTO TABLE customer_data SELECT * FROM staging_table;

- Sampling using buckets:
		SELECT * FROM customer_data TABLESAMPLE(BUCKET 1 OUT OF 4);


#Advantages of Bucketing:
			- Optimized join performance as buckets can be joined directly.
			- Efficient sampling for analysis on large datasets.

#Disadvantages:
			- Fixed number of buckets requires careful planning.
			- Data redistribution may be needed when bucket count changes.



3. Combining Partitioning and Bucketing
	Partitioning and bucketing can be combined for enhanced data organization. 
	Partitioning handles coarse-grained filtering, while bucketing ensures efficient processing within partitions.

#Example Query:
					CREATE TABLE sales_partitioned_bucketed (
						id INT,
						amount FLOAT
					)
					PARTITIONED BY (year INT)
					CLUSTERED BY (id) INTO 4 BUCKETS
					STORED AS PARQUET;

#Directory Structure:

			/user/hive/warehouse/sales_partitioned_bucketed/
			year=2024/
				000000_0
				000001_0
				000002_0
				000003_0
			year=2023/
				000000_0
				000001_0

Key Differences Between Partitioning and Bucketing

|---------------------------|--------------------------------------------|--------------------------------------------|
| Aspect                	| Partitioning                          	 | Bucketing                          	      |
|---------------------------|--------------------------------------------|--------------------------------------------|
| Data Organization     	| Based on partition column values.          | Based on hash of the bucketing column.     |
| Granularity           	| Coarse-grained filtering.                  | Fine-grained data distribution.            |
| Directory Structure   	| Creates separate directories per partition.| Creates files (buckets) within directories.|
| Use Case              	| Filtering and scanning specific partitions.| Sampling and join optimizations.           |
|---------------------------|--------------------------------------------|--------------------------------------------|

--------------------------------------------------------------------------------------------------------------
	Hive Optimization Techniques
--------------------------------------------------------------------------------------------------------------
Optimization techniques are crucial to improving query performance and resource utilization in Hive.

#1. Query Optimization
- Use Partition Pruning:
  Query only the relevant partitions using WHERE clauses.
  
  SELECT * FROM sales_data WHERE year = 2024 AND month = '01';
  
- Use Bucketing for Joins:
  Perform joins on bucketed tables to reduce shuffle and sort overhead.
  
  SELECT *
  FROM customer_data c
  JOIN orders_data o
  ON c.id = o.customer_id;
  

#2. File Format Optimization
	- Use columnar file formats like ORC or Parquet for better compression and faster scans.
	- Enable vectorized execution:
	SET hive.vectorized.execution.enabled = true;
  

#3. Compression
- Enable compression for intermediate and final outputs to save storage and improve I/O.
  
  SET hive.exec.compress.output = true;
  SET mapreduce.output.fileoutputformat.compress = true;
  SET mapreduce.map.output.compress = true;
  

#4. Tez Execution Engine
- Use the Tez execution engine for faster query processing:
  
  SET hive.execution.engine = tez;
  

#5. Partitioning and Bucketing
- Partition tables by frequently queried columns.
- Bucket data to distribute evenly for joins and sampling.

#6. Statistics Collection
- Gather table statistics for better query planning:
  
  ANALYZE TABLE sales_data COMPUTE STATISTICS;
  

#7. Avoid Small Files
- Consolidate small files using `hive.merge.*` properties:
  
  SET hive.merge.smallfiles.avgsize = 128000000;
  SET hive.merge.mapredfiles = true;
  

#8. Indexing
- Use indexing for faster data retrieval in selective queries:
  
  CREATE INDEX idx_customer ON TABLE customer_data (id)
  AS 'COMPACT'
  WITH DEFERRED REBUILD;
  

#9. Query Caching
- Cache frequently accessed query results to reduce processing time.

#10. Optimize Joins
- Use map-side joins for small tables:
  
  SELECT /*+ MAPJOIN(small_table) */ *
  FROM large_table
  JOIN small_table
  ON large_table.key = small_table.key;
  

--------------------------------------------------------------------------------------------------------------
	File Formats in Hive
--------------------------------------------------------------------------------------------------------------
Hive supports various file formats for storing data in tables. The choice of file format significantly impacts storage efficiency, query performance, and scalability. Below are the most common file formats used in Hive:

1. Text File
	Description:
		- The default storage format in Hive.
		- Data is stored as plain text with fields separated by a delimiter (e.g., comma, tab).

	Characteristics:
		- Easy to read and write.
		- Consumes more storage as there is no compression.
		- Slower query performance due to lack of indexing and metadata.

	Example:
				CREATE TABLE sales_data (
					id INT,
					amount FLOAT
				)
				ROW FORMAT DELIMITED
				FIELDS TERMINATED BY ','
				STORED AS TEXTFILE;

 2. Sequence File
		Description:
			- A flat-file format used for binary key-value pairs.
			- Supports compression, making it storage-efficient.

		Characteristics:
			- Faster than text files for reading and writing.
			- Suitable for intermediate MapReduce processing.

		Example:
				CREATE TABLE customer_data (
					id INT,
					name STRING
				)		
				STORED AS SEQUENCEFILE;
		
 3. RCFile (Row Columnar File)
		Description:
			- A columnar storage format introduced to optimize reading specific columns in queries.
			- Data is split into rows and stored in blocks.

		Characteristics:
			- Faster query performance for column-based operations.
			- Suitable for analytical queries.

		Example:
				CREATE TABLE employee_data (
					emp_id INT,
					emp_name STRING
				)
				STORED AS RCFILE;

 4. ORC (Optimized Row Columnar)

		Description:
			- Columnar storage format designed for Hive.
			- Stores metadata and supports indexing.

		Characteristics:
			- Highly compressed, reducing storage requirements.
			- Supports predicate pushdown, enhancing query performance.
			- Efficient for both read and write operations.

		Example:

				CREATE TABLE orders_data (
					order_id INT,
					order_date DATE
				)
				STORED AS ORC;


 5. Parquet
		Description:
			- A columnar format optimized for large-scale data processing.
			- Compatible with various data processing engines like Hive, Spark, and Impala.

		Characteristics:
			- Highly compressed.
			- Suitable for analytical workloads with frequent column-based queries.

		Example:
				CREATE TABLE transaction_data (
					txn_id INT,
					txn_amount FLOAT
				)
				STORED AS PARQUET;

 6. Avro
		Description:
			- A row-based format with schema evolution support.
			- Suitable for data exchange between systems.

		Characteristics:
			- Compact and efficient serialization.
			- Supports schema changes without impacting existing data.

		Example:
				CREATE TABLE product_data (
					product_id INT,
					product_name STRING
				)
				STORED AS AVRO;


 7. JSON
        Description:
			- Stores data in human-readable JSON format.
			- Ideal for semi-structured data.

		Characteristics:
			- High flexibility for schema-less data.
			- Slower performance compared to columnar formats.

		Example:

				CREATE TABLE logs_data (
					log_id INT,
					log_message STRING
				)
				STORED AS JSONFILE;

 
 Comparison of File Formats
 
|-------------------|-----------------|-----------------------|------------------------|-----------------------|------------------------------------|
| Format            | Compression 	  | Read Performance 	  | Write Performance	   | Schema Evolution 	   | Use Case                     	    |
|-------------------|-----------------|-----------------------|------------------------|-----------------------|------------------------------------| 
| Text File         | No              | Slow                  | Fast                   | No                    | Basic data storage and sharing.    |
| Sequence File     | Yes             | Medium                | Medium                 | No                    | Intermediate processing.           |
| RCFile            | Yes             | Medium                | Medium                 | No                    | Analytical queries.                |
| ORC               | Yes             | Fast                  | Medium                 | No                    | Large-scale analytics.             |
| Parquet           | Yes             | Fast                  | Medium                 | Yes                   | Columnar analytics.                |
| Avro              | Yes             | Medium                | Medium                 | Yes                   | Data exchange and schema evolution.|
| JSON              | No              | Slow                  | Medium                 | Yes                   | Semi-structured data.              |
|-------------------|-----------------|-----------------------|------------------------|-----------------------|------------------------------------|

 Choosing the Right File Format

- TextFile: Use for small, simple datasets or when compatibility is the priority.
- SequenceFile: Use for intermediate MapReduce jobs.
- RCFile: Use for legacy Hive systems with columnar queries.
- ORC: Best for Hive-specific workloads with high compression and query performance needs.
- Parquet: Best for systems that require interoperability and efficient columnar processing.
- Avro: Best for systems requiring schema evolution.
- JSON: Use for semi-structured or schema-less data.

------------------------------------------------------------------------------------------------------
 Hive Advanced Features: In-Depth Explanation
------------------------------------------------------------------------------------------------------
1. User-Defined Functions (UDFs)

		Description:
			- Hive allows users to write custom functions, called User-Defined Functions (UDFs), to handle complex logic not supported by built-in functions.
			- UDFs are written in Java (or Python using PyHive) and deployed to extend Hive’s functionality.
	
		Types of User-Defined Extensions:
			1. UDF (User-Defined Function): Operates on a single row of input and produces a single output.
			2. UDAF (User-Defined Aggregate Function): Performs aggregations across multiple rows (e.g., sum, average).
			3. UDTF (User-Defined Table Generating Function): Produces multiple rows from a single row of input.
	
		Steps to Create and Use a UDF:
			1. Write the UDF:
				- Example: A Java UDF to convert a string to uppercase.
				java
					public class UpperCaseUDF extends UDF {
						public Text evaluate(Text input) {
							if (input == null) return null;
							return new Text(input.toString().toUpperCase());
						}
					}
   
2. Compile and Deploy:
		- Compile the UDF and add the JAR to Hive.
			hive> ADD JAR /path/to/your_jar.jar;
   

3. Register and Use:
		CREATE TEMPORARY FUNCTION to_upper AS 'com.example.UpperCaseUDF';
		SELECT to_upper(name) FROM employee;
   
		Advantages:
			- Extends the capabilities of Hive for custom logic.
			- Allows reuse of existing business logic written in Java.

		Disadvantages:
			- Performance may vary based on implementation.
			- Requires Java/Python expertise for development.

2. Views and Materialized Views

		Views:
			- A view is a virtual table based on the result of a query. It doesn’t store data but simplifies complex queries by encapsulating them.

			Example:		
				CREATE VIEW employee_view AS
				SELECT emp_id, emp_name, emp_salary * 1.2 AS adjusted_salary
				FROM employee
				WHERE emp_salary > 3000;


			- Querying the view:
				SELECT * FROM employee_view;

			Advantages of Views:
				- Simplifies query development.
				- Enhances reusability and maintainability.

			Limitations:
				- Views are not optimized for performance as they compute results on-the-fly.
		
		Materialized Views:
				- Unlike regular views, materialized views store the query results physically on the disk for fast access.
				- Useful for frequently accessed queries with heavy computations.

			Example:
				CREATE MATERIALIZED VIEW mv_sales_summary
				AS
				SELECT year, SUM(sales_amount) AS total_sales
				FROM sales_data
				GROUP BY year;

		
			- To refresh the view after the base data changes:
				ALTER MATERIALIZED VIEW mv_sales_summary REBUILD;


			Advantages:
			- Improves query performance for repetitive queries.
			- Avoids recomputation by storing results.

			Limitations:
			- Requires storage space for materialized data.
			- Needs periodic refreshing to reflect updated data.


3. Transactions in Hive
	Description:
		- Hive supports ACID (Atomicity, Consistency, Isolation, Durability) properties for transactional workloads.
		- ACID enables insert, update, and delete operations in Hive tables.

	Prerequisites:
	1. Enable ACID properties in `hive-site.xml`:
		xml
			<property>
				<name>hive.txn.manager</name>
				<value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
			</property>
			<property>
				<name>hive.support.concurrency</name>
				<value>true</value>
			</property>
   
	2. Use `ORC` file format with `TBLPROPERTIES`.
		Example:
		CREATE TABLE employee_acid (
			emp_id INT,
			emp_name STRING,
			emp_salary FLOAT
			)
	STORED AS ORC
	TBLPROPERTIES ("transactional"="true");


	Transactional Operations:
		1. Insert:
			INSERT INTO employee_acid VALUES (1, 'John', 5000.0);
		2. Update:
			UPDATE employee_acid SET emp_salary = 5500.0 WHERE emp_id = 1;
		3. Delete: 
			DELETE FROM employee_acid WHERE emp_salary < 3000.0;
   

	Advantages:
		- Ensures data integrity for updates and deletes.
		- Enables multi-statement transactions with `BEGIN` and `COMMIT`.

	Disadvantages:
		- ACID tables require more storage and computational overhead.
		- Additional configuration is needed.



4. Hive on Spark and Tez

 Hive on Spark:
- Hive on Spark replaces MapReduce as the execution engine for Hive queries.
- Provides faster query execution through in-memory processing and DAG-based execution.

Steps to Enable Hive on Spark:
1. Set execution engine:
   
   SET hive.execution.engine=spark;
   
2. Ensure Spark is installed and configured with Hive.

Advantages:
- High performance for iterative queries.
- Optimized for machine learning workloads due to Spark’s in-memory capabilities.

Use Case:
- Use Hive on Spark for low-latency queries and machine learning preprocessing.


Hive on Tez:
- Tez is a generalized data flow programming framework.
- Provides faster execution for Hive queries compared to MapReduce by reducing disk I/O and reusing containers.

Steps to Enable Hive on Tez:
1. Set execution engine:
   
   SET hive.execution.engine=tez;
   
2. Ensure Tez libraries are available in the cluster.

Advantages:
- Faster execution for complex queries.
- Reduces query latency by minimizing intermediate writes to disk.

Use Case:
- Use Hive on Tez for large-scale ETL workflows and batch processing.

