Spark

1. Reading data in Spark
Dataframereader.format() ---->(datafile format. ex: csv, json, JDBC/ODBC, table, parquet.By default parquet fromat)
				.option("key", "value") ---->(inferschema,mode,header)
				.schema() ---->(manual schema you can pass)
				.load() ---->(path where file is residing)
(Dataframereader ----> Access it by "spark.read")

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("myspark").getOrCreate()

df = spark.read.format("csv")
				.option("header", "true")
				.option("inferschema", "true")
				.option("mode", "FAILFAST")
				.load("/path/to/input/file.csv")

2. How to create schema?
Method 1:-
	from pyspark.sql.types import StructType, StructField, IntegerType, StringType
	my_schema = StructType([
							StructField("id", IntegerType(), True),
							StructField("name", StringType(), True),
							StructField("age", IntegerType(), True)
						  ])
(True ----> accepts records with null values)

Method 2:-
	ddl_my_schema = "id integer, name string, age integer"

3. Handling Corrupt Records in Spark

To show corrupt records
	from pyspark.sql.types import StructType, StructField, IntegerType, StringType
	emp_schema = StructType([
							StructField("id", IntegerType(), True),
							StructField("name", StringType(), True),
							StructField("age", IntegerType(), True),
							StructField("salary", IntegerType(), True),
							StructField("address", StringType(), True),
							StructField("_corrupt_record", StringType(), True)
						  ])
	
	emp_df = spark.read.format("csv")
						.option("header", "true")
						.option("inferschema", "true")
						.option("mode", "PERMISSIVE")
						.schema(emp_schema)
						.load("/path/to/input/file.csv")
						
To Store Corrupt records
	emp_df = spark.read.format("csv")
						.option("header", "true")
						.option("inferschema", "true")
						.option("mode", "PERMISSIVE")
						.option("badRecordsPath", "/path/to/store/bad/records")
						.schema(emp_schema)
						.load("/path/to/input/file.csv")

4. How to read json file in spark?

For single line data :
	df = spark.read.format("json")
					.option("inferschema", "true")
					.option("mode", "PERMISSIVE")
					.load("/path/to/input/file.json")

For single line data with extra fields :
(the extra filed will be created as columns with null value in it)
	df = spark.read.format("json")
					.option("inferschema", "true")
					.option("mode", "PERMISSIVE")
					.load("/path/to/input/file.json")
					
For multiline data :
	df = spark.read.format("json")
					.option("inferschema", "true")
					.option("mode", "PERMISSIVE")
					.option("multiline", "true")
					.load("/path/to/input/file.json")

5. How to write dataframe into disk?
dataframeWriter.format()
				.option()
				.partitionBy()
				.bucketBy()
				.save

Modes in dataframe writer API:-
append
overwrite
error if exists
ignore

6. How to repartition?
df.repartition(3).write.format("csv")
						.option("header", "true")
						.option("mode", "overwrite")
						.option("path", "destination")
						.save()

7. Partition
df.write.format("csv")
		.option("header", "true")
		.option("mode", "overwrite")
		.option("path", "destination")
		.partitionBy("column_name")
		.save()
		
8. Bucket
df.write.format("csv")
		.option("header", "true")
		.option("mode", "overwrite")
		.option("path", "destination")
		.bucketBy(3, "id")
		.saveAsTable("table_name")

9. How to create df in spark?
my_data = [(1,1),(2,1),(3,1),(4,1),(5,1),(62,),(7,2)]
my_schema = ["id", "num"]

spark.createDataFrame(data=my_data, schema=my_schema)

10. "select" in different ways

from pyspark.sql.functions import *
from pyspark.sql.types import *

String method
	df.select("name").show() ----> String method.
	df.select("id", "name", "age").show() ----> select multiple column.
	df.select("id + 5").show() ----> Gives Analysis Exception as it searches column with name "id + 5".

Column method
	df.select(col("name")).show() ----> Column method
	df.select(col("name"),col("id")).show()
	df.select(col("id") + 5).show() ----> Searches column name "id" and add 5 to the value in that column.
	
df.select("*").show() 
df.select(df["salary"], df.salary) ----> mostly used for joins

df.select(expr("id + 5")).show() ----> Expression method is used to overcome Analysis Exception.
df.select(expr("id as emp_id"), expr("name as emp_name"), expr("concat(name, address))).show()

Spark SQL Query:-
	create data into temperory view
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select * from emp_table
				""").show()
	spark.sql("""
					select id, name, age from emp_table
				""").show()

11. Alias
df.select(col("id").alias("emp_id"), "name", "age").show() ----> To change the column name

12. Filter
df.filter(col("salary") > 15000).show()
df.where(col("salary") > 15000).show()
(both are the same)
df.filter((col("salary") > 15000) & (col("age")<18)).show()

Spark SQL Query:
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select * from emp_table
					where salary > 15000 and age < 18
				""").show()

13. Literals
It is used to add values in columns.
# Original Table:
+----+-------+
| id | name  |
+----+-------+
| 1  | Alice |
| 2  | Bob   |
+----+-------+

df.select("*", lit("kumar").alias("last_name")).show()
+----+-------+-----------+
| id | name  | last_name |
|----|-------|-----------|
| 1  | Alice | kumar     |
| 2  | Bob   | kumar     |
+----+-------+-----------+
lit("kumar"):The lit function creates a column with a constant value, in this case, the string "kumar".
alias("last_name"):Renames the constant column to "last_name".

df.select("*", lit("kumar")).show()
+---+-----+-----+
| id| name|  col|
+---+-----+-----+
|  1|Alice|kumar|
|  2|  Bob|kumar|
+---+-----+-----+

Spark SQL Query:
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select *,"kumar" as last_name from emp_table
					where salary > 15000 and age < 18
				""").show()

14. Adding Column
+----+-------+
| id | name  |
|----|-------|
| 1  | Alice |
| 2  | Bob   |
+----+-------+

df.withColumn("surname", lit("singh")) ----> Adds a new column named "surname" to the DataFrame.
											  The column will contain the constant value "singh" for all rows.
+---+-----+-------+
| id| name|surname|
+---+-----+-------+
|  1|Alice|  singh|
|  2|  Bob|  singh|
+---+-----+-------+

Spark SQL Query:
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select *,"kumar" as last_name, concat(name,last_name) as full_name from emp_table
					where salary > 15000 and age < 18
				""").show()

15. Renaming Column
df.withColumnRenamed("id", "emp_id")

Spark SQL Query:-
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select *,"id" as emp_id from emp_table
				""").show()

16. Casting Datatype
df.withColumn("id", col("id").cast("string"))
df.withColumn("id", col("id").cast("string")) \
	.withColumn("salary", col("salary").cast("long"))
	
Spark SQL Query:-
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select *,cast(id as string) emp_table
				""").show()
	
17. Removing Columns
df.drop("id", col("name"))

18. Union and UnionAll
df.count() --------> gives number of records
df1.union(df2) --------> merges 2 tables

Spark SQL Query:-
	df.createOrReplaceTempView("emp_table")
	spark.sql("""
					select * from df1
					union
					select * from df1
			 """).show()

df1.unionByName(df2) ----> if the sequence of two columns are different, it tries to match the column name.
df1.select("id","name","age","salary").union(df2) ----> For more column, union doesnt work.We can manually select
														columns to perform union of 2 tables.
19. If Else
df.withColumn("adult", when(col("age") < 18, "No")
					  .when(col("age") > 18, "Yes")
					  .otherwise("No Value"))

20. Unique and Sorted Records
df.distinct() ----> gets distinct records. No duplicates
df.select("column1","column2") ----> gets distinct records of the selected columns.
df.drop_duplicates("column1","column2") ----> drops duplicate records.
df.sort("sal") ----> String method.
df.sort(col("sal")) ---->  column method. Sorting in ascending.
df.sort(col("sal").desc) ----> column method. Sorting in descending.
df.sort(col("sal").desc(), col("name").desc()) ----> sorting multiple columns.


Aggregate functions
Collecting something together. It works specially on numeric values.

21. Count
df.count() ----> action
df.select(count("column1")) ----> transformation

22. Sum
df.select(sum("salary"), max("salary"), min("salary"))

23. Group By
df.groupBy("dept").agg(sum("salary"))
df.groupBy("dept", "country").agg(sum("salary"))

24. Join in Spark
df1.join(df2, join expression, jointype)
table1.join(table2, table2("column_name")==table1("column_name"), "inner")

-Types of Join:-
	inner join
	outer join(full outer join)
	left join
	right join
	left semi join (it only gives records of left table with common values)
	left anti join
	cross join (m*n)

25. Window function in Spark

from pyspark.sql import SparkSession
from pyspark.sql.functions import row_number, sum
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("WindowFunctions").getOrCreate()

window = Window.partitionBy("column1").orderBy("column1") ----> this is where window frame will be made
df = df.withColumn("row", row_number().over(window))
	row_number():-
		Assigns a unique, sequential number to each row within the window frame.
		Rows are numbered starting from 1.
		
row_number()
rank()
dense_rank()

df = df.withColumn("row", sum("column_name").over(window))

26. Lead
window = Window.partitionBy("product_id").orderBy("sales_date")

+------------+------------+-------+
| product_id | sales_date | sales |
+------------+------------+-------+
| A          | 2024-01-01 |   100 |
| A          | 2024-02-01 |   200 |
| A          | 2024-03-01 |   300 |
| B          | 2024-01-01 |   400 |
| B          | 2024-02-01 |   500 |
+------------+------------+-------+

next_month_df = df.withColumn("next_month_sales", lead(col("sales"), 1).over(window))
	lead(col("sales"), 1):-
		Looks 1 row ahead in the same partition to get the sales value for the next month.

+------------+------------+-------+------------------+
| product_id | sales_date | sales | next_month_sales |
+------------+------------+-------+------------------+
| A          | 2024-01-01 |   100 |              200 |
| A          | 2024-02-01 |   200 |              300 |
| A          | 2024-03-01 |   300 |             NULL |
| B          | 2024-01-01 |   400 |              500 |
| B          | 2024-02-01 |   500 |             NULL |
+------------+------------+-------+------------------+

27. Lag
window = Window.partitionBy("product_id").orderBy("sales_date")

+------------+------------+-------+
| product_id | sales_date | sales |
+------------+------------+-------+
| A          | 2024-01-01 |   100 |
| A          | 2024-02-01 |   200 |
| A          | 2024-03-01 |   300 |
| B          | 2024-01-01 |   400 |
| B          | 2024-02-01 |   500 |
+------------+------------+-------+


previous_month_df = df.withColumn("previous_month_sales", lag(col("sales"), 1).over(window))
	lead(col("sales"), 1):-
		Looks 1 row ahead in the same partition to get the sales value for the next month.

+------------+------------+-------+---------------------+
| product_id | sales_date | sales | previous_month_sales |
+------------+------------+-------+---------------------+
| A          | 2024-01-01 |   100 |                NULL |
| A          | 2024-02-01 |   200 |                 100 |
| A          | 2024-03-01 |   300 |                 200 |
| B          | 2024-01-01 |   400 |                NULL |
| B          | 2024-02-01 |   500 |                 400 |
+------------+------------+-------+---------------------+

28. Rows between in Spark
window = Window.partitionBy("column1").orderBy("column1")
			   .rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)

UNBOUNDED PRECEDING:-
	Refers to the first row in the partition.
	Includes all rows from the start of the partition up to the current row.
	Typically used in cumulative calculations (e.g., cumulative sum, average).
	
	from pyspark.sql.functions import sum
	from pyspark.sql.window import Window

	# Define a window with UNBOUNDED PRECEDING
	window_spec = Window.partitionBy("product_id")
						.orderBy("sales_date").rowsBetween(Window.unboundedPreceding, Window.currentRow)
	
	+------------+------------+-------+
	| product_id | sales_date | sales |
	+------------+------------+-------+
	| A          | 2024-01-01 |   100 |
	| A          | 2024-02-01 |   200 |
	| A          | 2024-03-01 |   300 |
	| B          | 2024-01-01 |   400 |
	| B          | 2024-02-01 |   500 |
	+------------+------------+-------+

	# Cumulative sum of sales
	df = df.withColumn("cumulative_sales", sum("sales").over(window_spec))
	
	+------------+------------+-------+------------------+
	| product_id | sales_date | sales | cumulative_sales |
	+------------+------------+-------+------------------+
	| A          | 2024-01-01 |   100 |              100 |
	| A          | 2024-02-01 |   200 |              300 |
	| A          | 2024-03-01 |   300 |              600 |
	| B          | 2024-01-01 |   400 |              400 |
	| B          | 2024-02-01 |   500 |              900 |
	+------------+------------+-------+------------------+


UNBOUNDED FOLLOWING:-
	Refers to the last row in the partition.
	Includes all rows from the current row to the end of the partition.
	Commonly used in calculations like finding maximums, sums, or averages across the entire partition.

	from pyspark.sql.functions import sum
	from pyspark.sql.window import Window

	# Define a window with UNBOUNDED FOLLOWING
	window_spec = Window.partitionBy("product_id")
						.orderBy("sales_date").rowsBetween(Window.currentRow, Window.unboundedFollowing)
	
	+------------+------------+-------+
	| product_id | sales_date | sales |
	+------------+------------+-------+
	| A          | 2024-01-01 |   100 |
	| A          | 2024-02-01 |   200 |
	| A          | 2024-03-01 |   300 |
	| B          | 2024-01-01 |   400 |
	| B          | 2024-02-01 |   500 |
	+------------+------------+-------+

	# Total sales from current row to the end of partition
	df = df.withColumn("total_sales", sum("sales").over(window_spec))
	
	+------------+------------+-------+-------------+
	| product_id | sales_date | sales | total_sales |
	+------------+------------+-------+-------------+
	| A          | 2024-01-01 |   100 |         600 |
	| A          | 2024-02-01 |   200 |         500 |
	| A          | 2024-03-01 |   300 |         300 |
	| B          | 2024-01-01 |   400 |         900 |
	| B          | 2024-02-01 |   500 |         500 |
	+------------+------------+-------+-------------+

29. Range between in Spark
The rangeBetween method is used in a window specification to define the boundaries of a window frame using 
logical ranges instead of row numbers. Unlike rowsBetween, which deals with physical rows, rangeBetween works 
with values in an ordered column (e.g., timestamps or numeric values).

Window.orderBy("column_name").rangeBetween(start, end)

window_spec = Window.partitionBy("product_id")
					.orderBy("sales_date").rangeBetween(Window.unboundedPreceding, Window.currentRow)
+------------+------------+-------+
| product_id | sales_date | sales |
+------------+------------+-------+
| A          | 2024-01-01 |   100 |
| A          | 2024-01-02 |   200 |
| A          | 2024-01-03 |   300 |
| B          | 2024-01-01 |   400 |
| B          | 2024-01-02 |   500 |
+------------+------------+-------+

# Calculate cumulative sales
df = df.withColumn("cumulative_sales", sum("sales").over(window_spec))
+------------+------------+-------+------------------+
| product_id | sales_date | sales | cumulative_sales |
+------------+------------+-------+------------------+
| A          | 2024-01-01 |   100 |              100 |
| A          | 2024-01-02 |   200 |              300 |
| A          | 2024-01-03 |   300 |              600 |
| B          | 2024-01-01 |   400 |              400 |
| B          | 2024-01-02 |   500 |              900 |
+------------+------------+-------+------------------+


window_spec = Window.partitionBy("product_id").orderBy("sales_date").rangeBetween(-1, 0)

# Calculate sum of sales in the range
df = df.withColumn("sales_range", sum("sales").over(window_spec))
+------------+------------+-------+-------------+
| product_id | sales_date | sales | sales_range |
+------------+------------+-------+-------------+
| A          | 2024-01-01 |   100 |         100 |
| A          | 2024-01-02 |   200 |         300 |
| A          | 2024-01-03 |   300 |         500 |
| B          | 2024-01-01 |   400 |         400 |
| B          | 2024-01-02 |   500 |         900 |
+------------+------------+-------+-------------+

30. Flatten Nested JSON
df.select("*", explode("column_name").alias("new_column_name"))

-explode function removes the entire row/records if there is any null values in that row.
-to solve this issue, we use "explode_outer" function.



































